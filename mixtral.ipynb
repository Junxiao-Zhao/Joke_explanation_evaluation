{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import regex as re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "eval_df = pd.read_csv('./data/eval-Mistral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\DL\\lib\\site-packages\\pyarrow\\pandas_compat.py:358: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "eval_ds = Dataset.from_pandas(eval_df[['joke', 'explanation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761b6fedb7364ddf9c0f67ef0922c8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbb2e7825da4f1787dbc435d9d44a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184ee2361e9f4081bf748202955f3868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1bdc6c785e402ab7c8a7bd9476ace9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "quan_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551eff616fa24db3b9389bf8ffc17daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897cf2333faa4a8f8fb34df3f53047d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195198b7df4b478dac198168826693db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a47228c6dda49a0ab2bb55a8bbfd995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4fd2da652b4a6aa80a9e60d43c9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114af5bdf9c46fd9829f5909264f58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0f0ae17b424638a3e902c334cd3d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1634c3945a4d4d269a915caea9b9ce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quan_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot\n",
    "prompt = \"\"\"<s>[INST]You are a joke explainer.\n",
    "\n",
    "Each input joke contains a question and an answer. Your job is to explain why the answer together with the question forms a joke.\n",
    "Use no more than 3-4 sentences to explain the joke. Do not add \"Ah I see\" or similar terms to your explanation.\n",
    "\n",
    "%s[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot\n",
    "prompt = \"\"\"<s>[INST]You are a joke explainer.\n",
    "\n",
    "Each input joke contains a question and an answer. Your job is to explain why the answer together with the question forms a joke.\n",
    "\n",
    "Here are some examples of joke explanations:\n",
    "Joke: Q: Why didn't the string ever win a race? A: It always tied!\n",
    "Explanation: The joke plays on the double meaning of the word 'tied.' In one sense, 'tied' means to finish a race at the same time as another competitor, resulting in a draw. In another sense, 'tied' refers to the action of tying a knot. Since the subject of the joke is a 'string,' the humor arises from imagining the string as always tying itself into knots, which humorously explains why it could never win a race.\n",
    "\n",
    "Use no more than 3-4 sentences to explain the joke. Do not add \"Ah I see\" or similar terms to your explanation.\n",
    "\n",
    "%s[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five-shot\n",
    "prompt = \"\"\"<s>[INST]You are a joke explainer.\n",
    "\n",
    "Each input joke contains a question and an answer. Your job is to explain why the answer together with the question forms a joke.\n",
    "\n",
    "Here are some examples of joke explanations:\n",
    "1.\n",
    "Joke: Q: Why didn't the string ever win a race? A: It always tied!\n",
    "Explanation: The joke plays on the double meaning of the word 'tied.' In one sense, 'tied' means to finish a race at the same time as another competitor, resulting in a draw. In another sense, 'tied' refers to the action of tying a knot. Since the subject of the joke is a 'string,' the humor arises from imagining the string as always tying itself into knots, which humorously explains why it could never win a race.\n",
    "2.\n",
    "Joke: Q: Why did the golfer wear two pairs of pants? A: In case he got a hole in one!\n",
    "Explanation: This joke plays on the double meaning of the phrase 'hole in one'. In golf, 'a hole in one' refers to a player hitting the ball directly from the tee into the hole with one stroke, which is a very good outcome. However, in the context of wearing pants, 'a hole in one' humorously suggests a literal hole in one of the pairs of pants. Thus, the golfer wearing two pairs of pants is a silly precaution to ensure he still has one intact pair if one gets a hole.\n",
    "3.\n",
    "Joke: Q:What do prisoners use to call each other? A:Cell phones!\n",
    "Explanation: The joke plays on the double meaning of the word 'cell.' In one sense, a 'cell' is a small room where prisoners are kept, and in another sense, it refers to 'cellular phones,' commonly known as cell phones. The humor arises from the pun created by using 'cell phones' to imply that prisoners would use phones related to their incarceration cells to communicate.\n",
    "4.\n",
    "Joke: Q: What kind of shoes does a ninja wear? A: Sneakers!\n",
    "Explanation: The joke plays on the word 'sneakers' which is a type of shoe known for being comfortable and quiet, ideal for casual wear. However, in the context of the joke, 'sneakers' also refers to the ability to sneak around silently, which is a stereotypical attribute of a ninja. Thus, the humor arises from the double meaning of 'sneakers' as both a type of shoe and a description of a ninja's stealthy behavior.\n",
    "5.\n",
    "Joke: Q: Why did Cinderella get kicked off the soccer team? A: Because she ran away from the ball!\n",
    "Explanation: The joke plays on the double meaning of the word 'ball'. In the context of the story of Cinderella, 'ball' refers to the formal dance she attends, from which she famously runs away at midnight. In the context of soccer, 'ball' refers to the soccer ball, which players should not run away from if they want to play effectively. The humor arises from the absurdity of applying the fairy tale behavior to the soccer field, creating a funny and unexpected reason for Cinderella to be kicked off the team.\n",
    "\n",
    "Use no more than 3-4 sentences to explain the joke. Do not add \"Ah I see\" or similar terms to your explanation.\n",
    "\n",
    "%s[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c43edfedd445aa50c892a6157bbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_ds = eval_ds.map(lambda x: {'prompt': prompt % x['joke']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = assist_pipeline(KeyDataset(eval_ds, 'prompt'),\n",
    "                          do_sample=True,\n",
    "                          top_k=10,\n",
    "                          num_return_sequences=1,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          max_length=1000,\n",
    "                          return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ans_ls = []\n",
    "for res in results:\n",
    "    ans_ls.append(re.sub(r'\\s+', ' ', res[0]['generated_text']).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['five-shot'] = ans_ls\n",
    "eval_df.to_csv('./data/eval-Mistral.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
